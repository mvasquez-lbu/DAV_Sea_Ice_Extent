{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('time_series_north_extent.xlsx')\n",
        "\n",
        "# Convert to Numpy array\n",
        "data['extent'] = np.asarray(data['extent'])\n",
        "data_np = data['extent']\n",
        "\n",
        "# Define the GRU model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.GRU(32, activation='relu', input_shape=(None, 1)),\n",
        "    tf.keras.layers.Dense(1)])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mae', optimizer=Adam(learning_rate=0.001), metrics=['mae'])\n",
        "\n",
        "# Training and Test datasets\n",
        "train_gru, val_gru = train_test_split(data_np, test_size=0.2,shuffle=False)\n",
        "\n",
        "history = model.fit(data_np[:-1], data_np[1:], epochs=200, batch_size=16,\n",
        "                    validation_split=0.2, callbacks=[early_stop])\n",
        "# Evaluate the model\n",
        "test_loss, test_mae = model.evaluate(val_gru)\n",
        "print(f'Test loss: {test_loss}, Test mse: {test_mae}')"
      ],
      "metadata": {
        "id": "P-oeO0LQECRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the loss iteration\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(history.history[\"mae\"], '-o', label=\"Training Loss\")  \n",
        "ax.plot(history.history[\"val_mae\"], '-r', label=\"Validation Loss\")\n",
        "ax.legend(loc='center right', fontsize=16)\n",
        "ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "ax.set_xlabel(\"Epoch\", fontsize=28)\n",
        "ax.set_ylabel(\"Loss\", fontsize=28)\n",
        "ax.set_title(\"Model Loss (MAE)\", fontsize=28)"
      ],
      "metadata": {
        "id": "1lLQVnLcH0WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions and validation values\n",
        "import plotly.graph_objects as go \n",
        "\n",
        "predictions = model.predict(val_gru)\n",
        "predictions = pd.DataFrame(predictions)\n",
        "train_gru_plot, val_gru_plot = train_test_split(data, test_size=0.2,\n",
        "                                                shuffle=False)\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.update_layout(width=800,height=600)\n",
        "fig.add_trace(go.Scatter(x=val_gru_plot['DATE'],y=val_gru_plot['extent'],\n",
        "                         name='Validation Data'))\n",
        "fig.add_trace(go.Scatter(x=val_gru_plot['DATE'],y=predictions[0],\n",
        "                         name='Prediction'))\n",
        "fig.update_layout(title='Sea Ice Extent Forecast - GRU',xaxis_title='Date',\n",
        "                  yaxis_title='Extent')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "fWLHhp8RLq87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,\n",
        " mean_absolute_percentage_error\n",
        "\n",
        "print(\"R2 score: %s\" %r2_score(val_gru_plot['extent'],predictions[0]))\n",
        "print(\"Mean Absolute Error: %s\" %mean_absolute_error(val_gru_plot['extent'],\n",
        "                                                     predictions[0]))\n",
        "print(\"MSE: %s\" %mean_squared_error(val_gru_plot['extent'],predictions[0]))\n",
        "print(\"RMSE: %s\" %mean_squared_error(val_gru_plot['extent'],predictions[0], \n",
        "                                     squared=False))\n",
        "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(\n",
        "    val_gru_plot['extent'],predictions[0]))\n"
      ],
      "metadata": {
        "id": "M_Wh-BM4sEdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "3Fm0vEqtLJZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "data_norm = pd.read_excel('time_series_north_extent.xlsx')\n",
        "\n",
        "# Normalize data\n",
        "data_reshape = data_norm['extent'].values.reshape(-1,1)\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform data using the scaler\n",
        "normalized_data = scaler.fit_transform(data_reshape)\n",
        "normalized_data_df = pd.DataFrame(normalized_data,columns=['extent'])\n",
        "data_norm['extent'] = normalized_data_df['extent']\n",
        "\n",
        "# Convert to Numpy Array\n",
        "data_norm['extent'] = np.asarray(data_norm['extent'])\n",
        "data_np_norm = data_norm['extent']\n",
        "\n",
        "# Define the GRU model\n",
        "model_norm = tf.keras.Sequential([\n",
        "    tf.keras.layers.GRU(32, activation='relu', input_shape=(None, 1)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
        "\n",
        "# Compile the model\n",
        "model_norm.compile(loss='mae', optimizer=Adam(learning_rate=0.001), \n",
        "                   metrics=['mae'])\n",
        "\n",
        "# Training and Test datasets\n",
        "train_gru_norm, val_gru_norm = train_test_split(data_np_norm,\n",
        "                                                test_size=0.2,shuffle=False)\n",
        "\n",
        "history_norm = model_norm.fit(data_np_norm[:-1], data_np_norm[1:], epochs=200, \n",
        "                  batch_size=16, validation_split=0.2, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_norm, test_mae_norm = model_norm.evaluate(val_gru_norm)\n",
        "\n",
        "print(f'Test loss: {test_loss_norm}, Test mse: {test_mae_norm}')"
      ],
      "metadata": {
        "id": "4LVcoGSvLNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the loss iteration\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(history_norm.history[\"mae\"], '-o', label=\"Training Loss\")  \n",
        "ax.plot(history_norm.history[\"val_mae\"], '-r', label=\"Validation Loss\")\n",
        "ax.legend(loc='center right', fontsize=16)\n",
        "ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "ax.set_xlabel(\"Epoch\", fontsize=28)\n",
        "ax.set_ylabel(\"Loss\", fontsize=28)\n",
        "ax.set_title(\"Model Loss (MAE)\", fontsize=28)"
      ],
      "metadata": {
        "id": "4hpABRlgOTJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions and validation values\n",
        "import plotly.graph_objects as go \n",
        "\n",
        "predictions_norm = model_norm.predict(val_gru_norm)\n",
        "predictions_norm = pd.DataFrame(predictions_norm)\n",
        "train_gru_plot_norm, val_gru_plot_norm = train_test_split(data_norm, \n",
        "                                                  test_size=0.2,shuffle=False)\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.update_layout(width=800,height=600)\n",
        "fig.add_trace(go.Scatter(x=val_gru_plot_norm['DATE'],\n",
        "                         y=val_gru_plot_norm['extent'],name='Validation Data'))\n",
        "fig.add_trace(go.Scatter(x=val_gru_plot_norm['DATE'],y=predictions_norm[0],\n",
        "                         name='Prediction'))\n",
        "fig.update_layout(title='Sea Ice Extent Forecast - GRU',xaxis_title='Date',\n",
        "                  yaxis_title='Extent')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "tiuKR1FUOcu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,\n",
        " mean_absolute_percentage_error\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"R2 score: %s\" %r2_score(val_gru_plot_norm['extent'],\n",
        "                               predictions_norm[0]))\n",
        "print(\"Mean Absolute Error: %s\" %mean_absolute_error(\n",
        "    val_gru_plot_norm['extent'],predictions_norm[0]))\n",
        "print(\"MSE: %s\" %mean_squared_error(val_gru_plot_norm['extent'],\n",
        "                                    predictions_norm[0]))\n",
        "print(\"RMSE: %s\" %mean_squared_error(val_gru_plot_norm['extent'],\n",
        "                                     predictions_norm[0], squared=False))\n",
        "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(\n",
        "    val_gru_plot_norm['extent'],predictions_norm[0]))\n"
      ],
      "metadata": {
        "id": "QWkRuLWetH6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}