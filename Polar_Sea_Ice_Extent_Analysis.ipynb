{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26603358",
   "metadata": {},
   "source": [
    "# Sea Ice extent Analysis Arctic and Antarctic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86502e7b",
   "metadata": {},
   "source": [
    "## Data cleaning North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "#loading the data\n",
    "data_north = pd.read_excel('N_monthly_extent.xlsx')\n",
    "\n",
    "data_north.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to data\n",
    "data_north.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd612a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive stats\n",
    "data_north.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922be7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data before imputation\n",
    "missing = data_north.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a499aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill data-type, area and extent with previous value\n",
    "data_north['data-type'].fillna(method=\"ffill\", inplace = True) \n",
    "data_north['extent'].fillna(method=\"ffill\", inplace = True)\n",
    "data_north['area'].fillna(method=\"ffill\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data after imputation\n",
    "missing = data_north.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive stats after imputation\n",
    "data_north.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9bf34",
   "metadata": {},
   "source": [
    "## Data cleaning South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats \n",
    "\n",
    "#loading the data\n",
    "data_south = pd.read_excel('S_monthly_extent.xlsx')\n",
    "\n",
    "data_south.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to data\n",
    "data_south.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ca2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive stats\n",
    "data_south.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data before imputation\n",
    "missing = data_south.isna().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill data-type, area and extent with previous value\n",
    "\n",
    "data_south['data-type'].fillna(method=\"ffill\", inplace = True) \n",
    "data_south['extent'].fillna(method=\"ffill\", inplace = True)\n",
    "data_south['area'].fillna(method=\"ffill\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data after imputation\n",
    "missing = data_south.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive stats after imputation\n",
    "data_south.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f30a7",
   "metadata": {},
   "source": [
    "## Level 1 Descriptive statistics North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45962",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measures of center ###\n",
    "\n",
    "df_north = pd.DataFrame(data_north,columns=['extent','area'])\n",
    "df_north.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c43e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot to investigate center of distribution\n",
    "\n",
    "df_extent_n_dist = pd.DataFrame(data_north,columns=['extent'])\n",
    "df_extent_n_dist.plot(kind=\"density\",\n",
    "              figsize=(10,10));\n",
    "\n",
    "plt.vlines(df_extent_n_dist.mean(),     # Plot black line at mean\n",
    "           ymin=0, \n",
    "           ymax=0.4,\n",
    "           linewidth=5.0,\n",
    "           color=\"black\"\n",
    "          );\n",
    "\n",
    "plt.vlines(df_extent_n_dist.median(),   # Plot red line at median\n",
    "           ymin=0, \n",
    "           ymax=0.4, \n",
    "           linewidth=2.0,\n",
    "           color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measures of spread ###\n",
    "# Interquartile (IQR) range is another common measure of spread. \n",
    "# IQR is the distance between the 3rd quartile and the 1st quartile\n",
    "\n",
    "df_extent_n_box = pd.DataFrame(data_north,columns=['extent','area'])\n",
    "\n",
    "df_extent_n_box.boxplot(column=[\"extent\"],\n",
    "               return_type='axes',\n",
    "               figsize=(8,8))\n",
    "\n",
    "plt.text(x=0.74, y=14, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=12, s=\"Median\")\n",
    "plt.text(x=0.75, y=8.3, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=3.5, s=\"Min\")\n",
    "plt.text(x=0.9, y=16.2, s=\"Max\")\n",
    "plt.text(x=0.7, y=11, s=\"IQR\", rotation=90, size=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurtosis and Skewness\n",
    "\n",
    "extent_n_skew = df_extent_n_box[\"extent\"].skew()\n",
    "area_n_skew = df_extent_n_box[\"area\"].skew()\n",
    "extent_n_kurt = df_extent_n_box[\"extent\"].kurt()\n",
    "area_n_kurt = df_extent_n_box[\"area\"].kurt()\n",
    "\n",
    "print(\"Skewness Extent: %s \\nKurtosis Extent: %s\" %(extent_n_skew,extent_n_kurt))\n",
    "print()\n",
    "print(\"Skewness Area: %s \\nKurtosis Area: %s\" %(area_n_skew,area_n_kurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sea ice extent throughout the years plot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "sns.lineplot(data=df_extent_n_plot, x=\"month\", y=\"extent\", hue=\"year\",palette= \"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and Maximum Sea Ice extent Plot\n",
    "\n",
    "df_extent_n_plot = pd.DataFrame(data_north,columns=['extent','year','month'])\n",
    "df_extent_n_min = df_extent_n_plot.loc[(df_extent_n_plot['month']==9)]\n",
    "df_extent_n_max = df_extent_n_plot.loc[(df_extent_n_plot['month']==3)]\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Minimum and Maximum Sea Ice Extent North', fontsize=20)\n",
    "axs[0].plot(df_extent_n_min['year'], df_extent_n_min['extent'],marker='o')\n",
    "axs[1].plot(df_extent_n_max['year'], df_extent_n_max['extent'], color='red', marker='o')\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='year', ylabel='extent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15c415",
   "metadata": {},
   "source": [
    "## Level 1 Descriptive Statistics South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c342c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measures of center ###\n",
    "\n",
    "df_south = pd.DataFrame(data_south,columns=['extent','area'])\n",
    "df_south.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot to investigate center of distribution\n",
    "\n",
    "df_extent_s_dist = pd.DataFrame(data_south,columns=['extent'])\n",
    "df_extent_s_dist.plot(kind=\"density\",\n",
    "              figsize=(10,10));\n",
    "\n",
    "plt.vlines(df_extent_s_dist.mean(),     # Plot black line at mean\n",
    "           ymin=0, \n",
    "           ymax=0.4,\n",
    "           linewidth=5.0,\n",
    "           color=\"black\"\n",
    "          );\n",
    "\n",
    "plt.vlines(df_extent_s_dist.median(),   # Plot red line at median\n",
    "           ymin=0, \n",
    "           ymax=0.4, \n",
    "           linewidth=2.0,\n",
    "           color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measures of spread ###\n",
    "# Interquartile (IQR) range is another common measure of spread. \n",
    "# IQR is the distance between the 3rd quartile and the 1st quartile\n",
    "\n",
    "df_extent_s_box = pd.DataFrame(data_south,columns=['extent'])\n",
    "\n",
    "df_extent_s_box.boxplot(column=[\"extent\"],\n",
    "               return_type='axes',\n",
    "               figsize=(8,8))\n",
    "\n",
    "plt.text(x=0.74, y=16.8, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=11.8, s=\"Median\")\n",
    "plt.text(x=0.75, y=6, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=2.0, s=\"Min\")\n",
    "plt.text(x=0.9, y=19.6, s=\"Max\")\n",
    "plt.text(x=0.7, y=10.8, s=\"IQR\", rotation=90, size=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurtosis and Skewness\n",
    "\n",
    "extent_s_skew = df_extent_s_box[\"extent\"].skew()\n",
    "area_s_skew = df_extent_s_box[\"area\"].skew()\n",
    "extent_s_kurt = df_extent_s_box[\"extent\"].kurt()\n",
    "area_s_kurt = df_extent_s_box[\"area\"].kurt()\n",
    "\n",
    "print(\"Skewness Extent: %s \\nKurtosis Extent: %s\" %(extent_s_skew,extent_s_kurt))\n",
    "print()\n",
    "print(\"Skewness Area: %s \\nKurtosis Area: %s\" %(area_s_skew,area_s_kurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f185d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sea ice extent throughout the years \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "df_extent_s_plot = pd.DataFrame(data_south,columns=['extent','year','month'])\n",
    "\n",
    "sns.lineplot(data=df_extent_s_plot, x=\"month\", y=\"extent\", hue=\"year\",palette= 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and Maximum sea ice extent plot\n",
    "\n",
    "df_extent_s_max = df_extent_s_plot.loc[(df_extent_s_plot['month']==9)]\n",
    "df_extent_s_min = df_extent_s_plot.loc[(df_extent_s_plot['month']==2)]\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Minimum and Maximum Sea Ice Extent South', fontsize=20)\n",
    "axs[0].plot(df_extent_s_min['year'], df_extent_s_min['extent'],marker='o')\n",
    "axs[1].plot(df_extent_s_max['year'], df_extent_s_max['extent'], color='red', marker='o')\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='year', ylabel='extent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037dc95",
   "metadata": {},
   "source": [
    "## Level 2 Inferential statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7e533",
   "metadata": {},
   "source": [
    "### T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7745ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test two sample North\n",
    "# Null hypothesis (H0): There is no significant difference in the mean ice extent loss between the first half of the \n",
    "# time period and the second half of the time period.\n",
    "# Alternative hypothesis (Ha): There is a significant difference in the mean ice extent loss between the first half \n",
    "# of the time period and the second half of the time period.\n",
    "import numpy as np\n",
    "\n",
    "start_date = 1980\n",
    "middle_date = 2001\n",
    "end_date = 2022\n",
    "\n",
    "df_extent_north = pd.DataFrame(data_north)\n",
    "\n",
    "# Select DataFrame rows between two dates for the first period\n",
    "first_period_north = (df_extent_north['year'] > start_date) & (df_extent_north['year'] <= middle_date)\n",
    "df_first_north = df_extent_north.loc[first_period_north]\n",
    "\n",
    "# Select DataFrame rows between two dates for the second period\n",
    "second_period_north = (df_extent_north['year'] > middle_date) & (df_extent_north['year'] <= end_date)\n",
    "df_second_north = df_extent_north.loc[second_period_north]\n",
    "\n",
    "# Create df for extent\n",
    "df_first_extent_north = df_first_north['extent']\n",
    "df_second_extent_north = df_second_north['extent']\n",
    "\n",
    "extent_by_period = pd.DataFrame({'first_p':df_first_extent_north,'second_p':df_second_extent_north})\n",
    "extent_by_period.describe()\n",
    "\n",
    "# Create t-test for groups\n",
    "t2,p = stats.ttest_rel(a = df_first_extent_north, b = df_second_extent_north)\n",
    "\n",
    "#two-tail 2-sample t-test\n",
    "alpha_half = 0.005 #alpha is 0.01 or level of confidence is 99%\n",
    "\n",
    "print(\"p value = {:g}\".format(p))\n",
    "print(\"t value = {:g}\". format(t2))\n",
    "\n",
    "if p < alpha_half:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis is accepted\")\n",
    "    \n",
    "# This is a test for the null hypothesis that two related or repeated samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test two sample South\n",
    "# Null hypothesis (H0): There is no significant difference in the mean ice extent loss between the first half of the \n",
    "# time period and the second half of the time period.\n",
    "# Alternative hypothesis (Ha): There is a significant difference in the mean ice extent loss between the first half \n",
    "# of the time period and the second half of the time period.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "start_date = 1980\n",
    "middle_date = 2001\n",
    "end_date = 2022\n",
    "\n",
    "df_extent_south = pd.DataFrame(data_south)\n",
    "\n",
    "# Select DataFrame rows between two dates for the first period\n",
    "first_period_south = (df_extent_south['year'] > start_date) & (df_extent_south['year'] <= middle_date)\n",
    "df_first_south = df_extent_south.loc[first_period_south]\n",
    "\n",
    "# Select DataFrame rows between two dates for the second period\n",
    "second_period_south = (df_extent_south['year'] > middle_date) & (df_extent_south['year'] <= end_date)\n",
    "df_second_south = df_extent_south.loc[second_period_south]\n",
    "\n",
    "# Create df for extent\n",
    "df_first_extent_south = df_first_south['extent']\n",
    "df_second_extent_south = df_second_south['extent']\n",
    "\n",
    "extent_by_period = pd.DataFrame({'first_p':df_first_extent_south,'second_p':df_second_extent_south})\n",
    "extent_by_period.describe()\n",
    "\n",
    "# Create t-test for groups\n",
    "t2,p = stats.ttest_rel(a = df_first_extent_south, b = df_second_extent_south)\n",
    "\n",
    "#two-tail 2-sample t-test\n",
    "alpha_half = 0.005 #alpha is 0.01 or level of confidence is 99%\n",
    "\n",
    "print(\"p value = {:g}\".format(p))\n",
    "print(\"t value = {:g}\". format(t2))\n",
    "\n",
    "if p < alpha_half:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis is accepted\")\n",
    "    \n",
    "# This is a test for the null hypothesis that two related or repeated samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bca38",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bead1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Way ANOVA: Test whether there's a significative difference between the minimum sea ice extent of 4 regions of the Arctic\n",
    "# Ho: All the means of the minimum sea ice extent of the different regions are the same\n",
    "# Hi: At least one mean differs from the rest\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load dataset\n",
    "data_regional = pd.read_excel('regional_extent_sept.xlsx')\n",
    "\n",
    "# Create groups\n",
    "beaufort = data_regional['Beaufort']\n",
    "bering = data_regional['Bering']\n",
    "central_arctic = data_regional['Central_Arctic']\n",
    "chukchi = data_regional['Chukchi']\n",
    "greenland = data_regional['Greenland']\n",
    "\n",
    "sns.boxplot(data=data_regional[['Beaufort', 'Central_Arctic', 'Bering', 'Chukchi', 'Greenland']])\n",
    "\n",
    "fvalue, pvalue = stats.f_oneway(beaufort, central_arctic, bering, chukchi, greenland)\n",
    "\n",
    "print(\"F Value  = {:g} \".format(fvalue))\n",
    "print(\"P Value  = {:g} \".format(pvalue))\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "if pvalue < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis is accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4288f",
   "metadata": {},
   "source": [
    "### Jan Mayen Greenland weather data and regional Greenland extent & area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecfdc0",
   "metadata": {},
   "source": [
    "### Correlation and Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#loading the data\n",
    "data = pd.read_csv('jnm_weather_station.csv')\n",
    "\n",
    "# Remove Date column\n",
    "df_heatmap = data.drop(['DATE','ELEVATION','LATITUDE','LONGITUDE','NAME','STATION'], axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df_heatmap.corr()\n",
    "display(corr)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.color_palette(\"magma\", as_cmap=True)#sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Matrix of TAVG and Greenland Sea Ice extent\n",
    "df_data = pd.DataFrame(data, columns=['extent_greenland','area_greenland','TAVG','PRCP'])\n",
    "pd.plotting.scatter_matrix(df_data,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc511c",
   "metadata": {},
   "source": [
    "## Level 3 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864425c",
   "metadata": {},
   "source": [
    "### Regression Models Greenland sea ice extent and Avg Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471941aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import requests, io\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loading the data\n",
    "data = pd.read_csv('jnm_weather_station.csv')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "data = data.drop(['DATE','ELEVATION','LATITUDE','LONGITUDE','NAME','STATION','area_greenland'], axis=1)\n",
    "\n",
    "# Normalize data using MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_norm = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Separating the features and the target variable\n",
    "x = data_norm['TAVG'].values.reshape(-1, 1) # Predictor\n",
    "y = data_norm['extent_greenland']  # Target\n",
    "\n",
    "# Splitting the dataset into training and testing set (80/20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31e2b2",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59240a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initializing the Random Forest Regression model with 100 decision trees\n",
    "model_rf = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "\n",
    "# Fitting the Random Forest Regression model to the data\n",
    "model_rf.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the target values of the test set\n",
    "y_pred_rf = model_rf.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(\"R2: %s\" %r2_rf)\n",
    "print(\"MSE: %s\" % mse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2aa38",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# creating a LinearRegression model\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "# fitting the training data\n",
    "model_lr.fit(x_train,y_train)\n",
    "\n",
    "# Predicting the target values of the test set\n",
    "y_pred_lr =  model_lr.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(\"R2: %s\" %r2_lr)\n",
    "print(\"MSE: %s\" % mse_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e27582",
   "metadata": {},
   "source": [
    "#### Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded39798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create SVR Model\n",
    "model_svr = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "\n",
    "# Fit the SVR model using the training data\n",
    "model_svr.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the target values of the test set\n",
    "y_pred_svr = model_svr.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(\"R2: %s\" %r2_svr)\n",
    "print(\"MSE: %s\" % mse_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5426e",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create Gradient Boosting model\n",
    "model_gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model_gb.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred_gb = model_gb.predict(x_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(\"R2: %s\" %r2_gb)\n",
    "print(\"MSE: %s\" % mse_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the predicted values against the true values\n",
    "plt.scatter(y_test, y_pred_lr,label='Linear Regression')\n",
    "plt.scatter(y_test, y_pred_gb,label='Gradient Boost')\n",
    "plt.scatter(y_test, y_pred_svr, label='Support Vector')\n",
    "plt.scatter(y_test, y_pred_rf,label='Random Forest')\n",
    "\n",
    "# Add a regression line to the plot\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--k', linewidth=2)\n",
    "\n",
    "# Set the x- and y-axis labels and the plot title\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Gradient Boosting Regressor Model')\n",
    "\n",
    "# Set the x- and y-axis labels and the plot title\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Regression Model Comparisons')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94475aa",
   "metadata": {},
   "source": [
    "###  Arctic Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8bdfe",
   "metadata": {},
   "source": [
    "### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from pylab import rcParams\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data_time_series_sar = pd.read_excel('time_series_north_extent.xlsx',index_col=0)\n",
    " \n",
    "# Convert to date time format\n",
    "data_time_series_sar.index = pd.to_datetime(data_time_series_sar.index)\n",
    "\n",
    "# Plot Sea Ice extent\n",
    "data_time_series_sar.plot(figsize=(15, 6))\n",
    "plt.gca().set(title=\"Sea Ice extent North 1979-2022\", xlabel=\"Date\", ylabel=\"Extent\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Seasonal decomposition: We can clearly see a seasonal pattern in the data as the extent tends to reach its \n",
    "# minimum in September and its maximum during the month of March\n",
    "\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "decomposition = sm.tsa.seasonal_decompose(data_time_series_sar, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training and Test datasets\n",
    "train_sar, val_sar = train_test_split(data_time_series_sar, test_size=0.2,shuffle=False)\n",
    "\n",
    "# SARIMA MODEL\n",
    "model = auto_arima(data_time_series_sar['extent'], start_p=1, start_q=1,max_p=3, max_q=3, m=12,start_P=1, seasonal= True)\n",
    "auto_arima(data_time_series_sar['extent'], start_p=1, start_q=1,max_p=3, max_q=3, m=12,start_P=1, seasonal= True).summary()\n",
    "\n",
    "print(\"ARIMA order:\", model.order)\n",
    "print(\"Seasonal order:\", model.seasonal_order)\n",
    "\n",
    "# Predictions\n",
    "sarima_result = model.fit(train_sar)\n",
    "#arima_result.summary()\n",
    "prediction_sar = sarima_result.predict(n_periods=len(val_sar)).rename('SARIMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Forecast vs Validation values\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "prediction_sar_df = pd.DataFrame(prediction_sar,columns=['SARIMA'])\n",
    "prediction_sar_df.reset_index(inplace=True)\n",
    "prediction_sar_df = prediction_sar_df.rename(columns={'index':'DATE'})\n",
    "\n",
    "train_sar_plot = train_sar\n",
    "train_sar_plot.reset_index(inplace=True)\n",
    "\n",
    "val_sar_plot = val_sar\n",
    "val_sar_plot.reset_index(inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_sar_plot['DATE'],y=train_sar_plot['extent'],name='Train'))\n",
    "fig.add_trace(go.Scatter(x=val_sar_plot['DATE'],y=val_sar_plot['extent'],name='Validation'))\n",
    "fig.add_trace(go.Scatter(x=prediction_sar_df['DATE'],y=prediction_sar_df['SARIMA'],name='Prediction'))\n",
    "fig.update_layout(title='Sea Ice Extent Forecast - SARIMA',xaxis_title='Date',yaxis_title='Extent')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d05176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"R2 score: %s\" %r2_score(val_sar['extent'],prediction_sar_df['SARIMA']))\n",
    "print(\"Mean Absolute Error: %s\" %mean_absolute_error(val_sar['extent'],prediction_sar_df['SARIMA']))\n",
    "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(val_sar['extent'],prediction_sar_df['SARIMA']))\n",
    "print(\"MSE: %s\" %mean_squared_error(val_sar['extent'],prediction_sar_df['SARIMA']))\n",
    "print(\"RMSE: %s\" %mean_squared_error(val_sar['extent'],prediction_sar_df['SARIMA'], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbd156",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prophet.plot import plot_plotly, plot_components_plotly \n",
    "\n",
    "# load dataset\n",
    "data_time_series_pr = pd.read_excel('time_series_north_extent.xlsx')\n",
    "\n",
    "# Convert to date time format\n",
    "data_time_series_pr['DATE'] = pd.to_datetime(data_time_series_pr['DATE'])\n",
    "\n",
    "# Checking datatype again to see if the data type is time series now\n",
    "print(type(data_time_series_pr.DATE[0]))\n",
    "\n",
    "# Set indexing to timestamp\n",
    "data_time_series_pr.sort_values(by='DATE')\n",
    "\n",
    "# Rename columns (Prophet requirement)\n",
    "data_time_series_pr = data_time_series_pr.rename(columns={'DATE': 'ds','extent': 'y'})\n",
    "\n",
    "# Training and Test datasets\n",
    "train_pr, val_pr = train_test_split(data_time_series_pr, test_size=0.2,shuffle=False)\n",
    "\n",
    "# Fit model\n",
    "m = Prophet(seasonality_mode='additive')\n",
    "m.fit(train_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7637602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for future dates to predict and show the forecast\n",
    "\n",
    "future = m.make_future_dataframe(periods=len(val_pr),include_history=False,freq='M')\n",
    "forecast=m.predict(future)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Forecast \n",
    "\n",
    "plot_plotly(m,forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Forecast vs Validation values\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_pr['ds'],y=train_pr['y'],name='Train'))\n",
    "fig.add_trace(go.Scatter(x=val_pr['ds'],y=val_pr['y'],name='Validation'))\n",
    "fig.add_trace(go.Scatter(x=forecast['ds'],y=forecast['yhat'],name='Prediction'))\n",
    "fig.update_layout(title='Sea Ice Extent Forecast - Prophet',xaxis_title='Date',yaxis_title='Extent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ddb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track changes in the trend line\n",
    "\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "fig=m.plot(forecast)\n",
    "a=add_changepoints_to_plot(fig.gca(),m,forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"R2 score: %s\" %r2_score(val_pr['y'],forecast['yhat']))\n",
    "print(\"Mean Absolute Error: %s\" %mean_absolute_error(val_pr['y'],forecast['yhat']))\n",
    "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(val_pr['y'],forecast['yhat']))\n",
    "print(\"MSE: %s\" %mean_squared_error(val_pr['y'],forecast['yhat']))\n",
    "print(\"RMSE: %s\" %mean_squared_error(val_pr['y'], forecast['yhat'],squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83489747",
   "metadata": {},
   "source": [
    "### Holt-Winters Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Load Dataset\n",
    "data_time_series_hw = pd.read_excel('time_series_north_extent.xlsx',index_col=0)\n",
    "\n",
    "# Convert to date time format\n",
    "data_time_series_hw.index = pd.to_datetime(data_time_series_hw.index)\n",
    "\n",
    "# Training and Test datasets\n",
    "train_hw, val_hw = train_test_split(data_time_series_hw, test_size=0.2,shuffle=False)\n",
    "\n",
    "# Create model and make predictions\n",
    "fitted_model = ExponentialSmoothing(train_hw['extent'],seasonal=\"mul\").fit()\n",
    "test_predictions = fitted_model.predict(start='2014-03-01',end='2022-12-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Forecast vs Validation values\n",
    "import plotly.graph_objects as go \n",
    "\n",
    "prediction_hw_df = pd.DataFrame(test_predictions,columns=['HW'])\n",
    "prediction_hw_df.reset_index(inplace=True)\n",
    "prediction_hw_df = prediction_hw_df.rename(columns={'index':'DATE'})\n",
    "\n",
    "train_hw_plot = train_hw\n",
    "train_hw_plot.reset_index(inplace=True)\n",
    "\n",
    "val_hw_plot = val_hw\n",
    "val_hw_plot.reset_index(inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train_hw_plot['DATE'],y=train_hw_plot['extent'],name='Train'))\n",
    "fig.add_trace(go.Scatter(x=val_hw_plot['DATE'],y=val_hw_plot['extent'],name='Validation'))\n",
    "fig.add_trace(go.Scatter(x=prediction_hw_df['DATE'],y=prediction_hw_df['HW'],name='Prediction'))\n",
    "fig.update_layout(title='Sea Ice Extent Forecast - Holt-Winters',xaxis_title='Date',yaxis_title='Extent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"R2 score: %s\" %r2_score(val_hw['extent'],test_predictions))\n",
    "print(\"Mean Absolute Error: %s\" %mean_absolute_error(val_hw['extent'],test_predictions))\n",
    "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(val_hw['extent'],test_predictions))\n",
    "print(\"MSE: %s\" %mean_squared_error(val_hw['extent'],test_predictions))\n",
    "print(\"RMSE: %s\" %mean_squared_error(val_hw['extent'],test_predictions, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb6de",
   "metadata": {},
   "source": [
    "## Level 4 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3813d",
   "metadata": {},
   "source": [
    "### NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# load dataset\n",
    "data_time_series_npr = pd.read_excel('time_series_north_extent.xlsx')\n",
    "\n",
    "# Convert to date time format\n",
    "data_time_series_npr['DATE'] = pd.to_datetime(data_time_series_npr['DATE'])\n",
    "\n",
    "# Checking datatype again to see if the data type is time series now\n",
    "print(type(data_time_series_npr.DATE[0]))\n",
    "\n",
    "# Set indexing to timestamp\n",
    "data_time_series_npr.sort_values(by='DATE')\n",
    "\n",
    "# Rename columns (NeuralProphet requirement)\n",
    "data_time_series_npr = data_time_series_npr.rename(columns={'DATE': 'ds','extent': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and fit it to the data, validation split 20%\n",
    "m = NeuralProphet()\n",
    "df_train, df_val = m.split_df(data_time_series_npr, freq='M', valid_p = 0.2)\n",
    "metrics = m.fit(df_train, freq='M', validation_df=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822be253",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for forecasting \n",
    "\n",
    "future = m.make_future_dataframe(df_val,n_historic_predictions=True)\n",
    "forecast = m.predict(df_val)\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ea2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Model Loss\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(metrics[\"MAE\"], '-o', label=\"Training Loss\")  \n",
    "ax.plot(metrics[\"MAE_val\"], '-r', label=\"Validation Loss\")\n",
    "ax.legend(loc='center right', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=28)\n",
    "ax.set_ylabel(\"Loss\", fontsize=28)\n",
    "ax.set_title(\"Model Loss (MAE)\", fontsize=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c575b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "print(\"R2 score: %s\" %r2_score(df_val['y'],forecast['yhat1']))\n",
    "print(\"Mean Absolute Error: %s\" %mean_absolute_error(df_val['y'],forecast['yhat1']))\n",
    "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(df_val['y'],forecast['yhat1']))\n",
    "print(\"MSE: %s\" %mean_squared_error(df_val['y'],forecast['yhat1']))\n",
    "print(\"RMSE: %s\" %mean_squared_error(df_val['y'],forecast['yhat1'], squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1088577",
   "metadata": {},
   "source": [
    "# Tests with Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d5903",
   "metadata": {},
   "source": [
    "## NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ec56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from neuralprophet import NeuralProphet\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load dataset\n",
    "data_time_series_npr_norm = pd.read_excel('time_series_north_extent.xlsx')\n",
    "\n",
    "# Normalize data\n",
    "data = data_time_series_npr_norm['extent'].values.reshape(-1,1)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform data using the scaler\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "normalized_data_df = pd.DataFrame(normalized_data,columns=['extent'])\n",
    "data_time_series_npr_norm['extent'] = normalized_data_df['extent']\n",
    "\n",
    "# Convert to date time format\n",
    "data_time_series_npr_norm['DATE'] = pd.to_datetime(data_time_series_npr_norm['DATE'])\n",
    "\n",
    "# Checking datatype again to see if the data type is time series now\n",
    "print(type(data_time_series_npr_norm.DATE[0]))\n",
    "\n",
    "# Set indexing to timestamp\n",
    "data_time_series_npr_norm.sort_values(by='DATE')\n",
    "\n",
    "# Rename columns (NeuralProphet requirement)\n",
    "data_time_series_npr_norm = data_time_series_npr_norm.rename(columns={'DATE': 'ds','extent': 'y'})\n",
    "\n",
    "# Create model \n",
    "m_norm = NeuralProphet()\n",
    "df_train_norm, df_val_norm = m_norm.split_df(data_time_series_npr_norm, freq='M', valid_p = 0.2)\n",
    "metrics_norm = m_norm.fit(df_train_norm, freq='M', validation_df=df_val_norm)\n",
    "\n",
    "# Predict for the last 20% of the time series\n",
    "display(metrics_norm.tail())\n",
    "future_norm = m_norm.make_future_dataframe(df_val_norm,n_historic_predictions=True)\n",
    "forecast_norm = m_norm.predict(df_val_norm)\n",
    "m_norm.plot(forecast_norm)\n",
    "\n",
    "# Plot Model Loss\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "ax.plot(metrics_norm[\"MAE\"], '-o', label=\"Training Loss\")  \n",
    "ax.plot(metrics_norm[\"MAE_val\"], '-r', label=\"Validation Loss\")\n",
    "ax.legend(loc='center right', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=28)\n",
    "ax.set_ylabel(\"Loss\", fontsize=28)\n",
    "ax.set_title(\"Model Loss (MAE)\", fontsize=28)\n",
    "fig.show()\n",
    "\n",
    "# Evaluation Metrics\n",
    "print(\"R2 score: %s\" %r2_score(df_val_norm['y'],forecast_norm['yhat1']))\n",
    "print(\"Mean Absolute Error: %s\" %mean_absolute_error(df_val_norm['y'],forecast_norm['yhat1']))\n",
    "print(\"MSE: %s\" %mean_squared_error(df_val_norm['y'],forecast_norm['yhat1']))\n",
    "print(\"RMSE: %s\" %mean_squared_error(df_val_norm['y'],forecast_norm['yhat1'], squared=False))\n",
    "print(\"Mean Absolute Percentage Error: %s\" %mean_absolute_percentage_error(df_val_norm['y'],forecast_norm['yhat1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec087da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
